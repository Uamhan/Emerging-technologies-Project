{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Digit recognition\n",
    "\n",
    "In this note book we will cover the digit recognition python script also found in this repositiory. we will first break the script down into three sections. first the machine learning section, which trainings the model used to make the predictions. the second section will be how we get the users image and manipulate it and third will be the user interface generated by the script."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine learning\n",
    "\n",
    "This section will discuss the the main bulk of the functional code in this script. all of this code is containted withing the predict_image method that takes the user image as input. this method imports the mnist data set from the keras library into testing and training sets to be used with the model we create. This section is broken down into three main components the model that we will train. the training section itself and finaly the predicition process.This will be concluded by discussing the performance of the model and predicitions.\n",
    "\n",
    "#### Model\n",
    "\n",
    "The first step in our machine learning predicition is to create/load the Model we will be using to make predicitions.\n",
    "At first the script will try to load a pre existing model. if it is not present it will create the model.\n",
    "\n",
    "This model is a keras sequential model this tells us that the layers are in a linear stack. i have decided to go with keras Conv2d Layers predominatly for this model (convolutional layer specificaly for 2d images)as they are very effective in visual based scenarios.\n",
    "\n",
    "The first layer of our model is a conv2D layer with 32 nodes. a kernal size of (3,3) which refers to the width x height of the filter mask. it has its activation function set to relu this stands for Rectifier or rectified linear unit this activation function is used comonly in computer vision tasks. finaly it has an input shape of (28,28,1) whitch models out mnist data set image dimensions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=input_shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the next layer is much the same but this time the conv2D layer has 64 nodes instead of 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Conv2D(64, (3, 3), activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply a max pooling of (2,2) which reduces our output shape from the previos layer from (3,3) to (2,2) taking the max pixel values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(MaxPooling2D(pool_size=(2, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we then apply a dropout of 0.25 this helps prevent overfitting to out training set by droping out random nodes from our model at a 25% chance ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we then flatten the current layer back down to a one dimensional array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we then add a dense layer with 128 nodes dense layers are layers where all of the inputs are connected to all of the outputs. it also uses the same relu activation as discussed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Dense(128, activation='relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then apply another dropout rate of 50% this time.This reduces the amount of weights in the dense layer by 50 percent as we are droping half of the nodes randomly. this is helpful as in a dense layer we have weights equal to the number of outputs times the number of inputs which can become very large. the dropout rate here not only prevents over fitting it greatly increases the efficency of our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Dropout(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finaly we have the last dense layer this time with on node per desired output in this case it will be 10 as the model will be predicting a number between 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.add(Dense(10,activation=tf.nn.softmax))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we then need to complile the model. we are using adam optimiser with the spare categorical creossentropy for our loss function with the solo metric of accuracy. I chose the adam optimisation as its Computationally efficient and well suited for problems that are large in terms of data and/or parameters. since we are useing images for our inputs even at the reduced size of 28x28 pixels these advantages are incredibly useful. our taget predictions are integers and not one hot codeds so spare_categorical_crossentropy is the logical choice fot the loss function and we have chose to simply display accuracy of the prediction as our only metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then fit our MNIST training data that we imported from the keras library to the model. we have decided to go with 10 iterations of the fitting to maxamise the accuracy of our results any thing more than 10 has seemed to produce neglibles results in this given problem. we will be using the test data we imported from the keras library as our validation set. and finaly we save the model weights so the user will only have to train the model once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.fit(data_training, label_training,epochs=10,validation_data=(data_test, label_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicitions\n",
    "\n",
    "To make predictions we simply convert the user input image to a numpy array and reshape it the (1,28,28,1) so as to match the values the model was trained on.  the predict method will return an array with 10 values. the index of the value represents the number it is predicting. the values in the array represent the likelyhood of that result. so a simple argmax call on the prediciton array will return the most likely predicition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converts user inputed image\n",
    "npImageArray = np.array(userimage)\n",
    "#predicts the digit from user image using the model\n",
    "prediction = model.predict(npImageArray.reshape(1,28,28,1))\n",
    "#returns predicted value\n",
    "return(prediction.argmax())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performance\n",
    "\n",
    "The main metric for performace we will be using for this model is the accuracy of the predicition as ultimately the goal of a digit recognition program is to be able to correctly recognise the digits. in the image bellow we will see the loss and accuracy values for each epoch of the training period. as we can see after completion of the tenth epoch we have an accuracy of 0.9831 or 98.31% while a result such as this may not be suficent for the likes of a self driving car as lifes can depend on it for the task of visual recognition a 98% accuracy is satisfactory.\n",
    "\n",
    "<img src=\"img/epoch.JPG\" width=\"600\" height=\"400\" align=\"left\"  />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a mid teir intel cpu each epoch took roughly 2 and half minutes leading to a 25 minute training time with all ten epochs the epoch value in the script can be changed to reduce the training time at a cost to accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Image\n",
    "\n",
    "This section will cover the method the script uses of inporting the users image and how it preps this image for the predicition process.\n",
    "\n",
    "the prep_image method takes in an image converts it to gray scale resizes the image to 28x28 pixels to match the mnist dataset we then flatten the image from 0-255 shades between white and black to two shades white or black. it then returns this modifyed image.\n",
    "\n",
    "we then have the select image function. this is the function that will be called by the button in the user interface as will be discused in the following section. this function will access a pannel that the image will be displayed on. the path of the image will be taken from tkinters filedialog method that will open the windows explore and the user may select there image. it will then check to see if the path is not empty. as would be the case if the user clicked cancel instead of the image they wanted. we import the image from that path into our image varibale. we then convert the image from BGR to RGB witch are two difrent colour formats and we need the image to be in RGB to display it properly for the user. we then call the prep_image method as discussed above this preped image is then passed to the predicition method also discussed above. the prediction is printed to the console. the image panel is then instantiated if it is not already. The display image is set to the user image so we can see the image and the scripts prediciton at the same time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Interface\n",
    "\n",
    "For this script we have written a very simply user inteface that simply consists of a select image button that will allow the user to select an image using the OS file explorer. this image will then be preped for predicition used with the model to get a predicition which will be printed to the console and the userinterface will display the image the user selected."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
